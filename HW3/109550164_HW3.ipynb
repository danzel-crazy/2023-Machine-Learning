{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW3: Decision Tree, AdaBoost and Random Forest\n",
    "In hw3, you need to implement decision tree, adaboost and random forest by using only numpy, then train your implemented model by the provided dataset. TA will use the on-hold test label to evaluate your model performance.\n",
    "\n",
    "Please note that only **NUMPY** can be used to implement your model, you will get no points by simply calling `sklearn.tree.DecisionTreeClassifier`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "Gini Index or Entropy is often used for measuring the “best” splitting of the data. Please compute the Entropy and Gini Index of provided data. Please use the formula from [page 5 of hw3 slides](https://docs.google.com/presentation/d/1kIe_-YZdemRMmr_3xDy-l0OS2EcLgDH7Uan14tlU5KE/edit#slide=id.gd542a5ff75_0_15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute gini \n",
    "def gini(sequence):\n",
    "    c1, c2 = 0, 0\n",
    "    for i in sequence:\n",
    "        if i == 1:\n",
    "            c1 += 1\n",
    "        else :\n",
    "            c2 += 1\n",
    "    \n",
    "    return 1 - pow((c1/len(sequence)), 2) - pow((c2/len(sequence)), 2)\n",
    "\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "#compute entropy\n",
    "def entropy(sequence):\n",
    "    c1, c2 = 0, 0\n",
    "    for i in sequence:\n",
    "        if i == 1:\n",
    "            c1 += 1\n",
    "        else :\n",
    "            c2 += 1\n",
    "    \n",
    "    entro = - (((c1/len(sequence)) * math.log2(c1/len(sequence))) + ((c2/len(sequence)) * math.log2(c2/len(sequence))))\n",
    "    return entro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 = class 1,\n",
    "# 2 = class 2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "data = np.array([1,2,1,1,1,1,2,2,1,1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gini of data is  0.4628099173553719\n"
     ]
    }
   ],
   "source": [
    "print(\"Gini of data is \", gini(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy of data is  0.9456603046006402\n"
     ]
    }
   ],
   "source": [
    "print(\"Entropy of data is \", entropy(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "It is a binary classifiation dataset that classify if price is high or not for a cell phone, the label is stored in `price_range` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1200, 21)\n",
      "(300, 21)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>battery_power</th>\n",
       "      <th>blue</th>\n",
       "      <th>clock_speed</th>\n",
       "      <th>dual_sim</th>\n",
       "      <th>fc</th>\n",
       "      <th>four_g</th>\n",
       "      <th>int_memory</th>\n",
       "      <th>m_dep</th>\n",
       "      <th>mobile_wt</th>\n",
       "      <th>n_cores</th>\n",
       "      <th>...</th>\n",
       "      <th>px_height</th>\n",
       "      <th>px_width</th>\n",
       "      <th>ram</th>\n",
       "      <th>sc_h</th>\n",
       "      <th>sc_w</th>\n",
       "      <th>talk_time</th>\n",
       "      <th>three_g</th>\n",
       "      <th>touch_screen</th>\n",
       "      <th>wifi</th>\n",
       "      <th>price_range</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1583</td>\n",
       "      <td>1</td>\n",
       "      <td>2.1</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>0.7</td>\n",
       "      <td>148</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>942</td>\n",
       "      <td>1651</td>\n",
       "      <td>1704</td>\n",
       "      <td>17</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>745</td>\n",
       "      <td>1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>35</td>\n",
       "      <td>0.8</td>\n",
       "      <td>102</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>89</td>\n",
       "      <td>1538</td>\n",
       "      <td>2459</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>832</td>\n",
       "      <td>0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>39</td>\n",
       "      <td>0.7</td>\n",
       "      <td>103</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>125</td>\n",
       "      <td>1504</td>\n",
       "      <td>1799</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1175</td>\n",
       "      <td>1</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>0.3</td>\n",
       "      <td>164</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>873</td>\n",
       "      <td>1394</td>\n",
       "      <td>1944</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>695</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>0.6</td>\n",
       "      <td>196</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>1649</td>\n",
       "      <td>1829</td>\n",
       "      <td>2855</td>\n",
       "      <td>16</td>\n",
       "      <td>13</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   battery_power  blue  clock_speed  dual_sim  fc  four_g  int_memory  m_dep  \\\n",
       "0           1583     1          2.1         1  11       0          14    0.7   \n",
       "1            745     1          0.6         1   5       0          35    0.8   \n",
       "2            832     0          0.7         1   2       1          39    0.7   \n",
       "3           1175     1          1.3         0   2       0          19    0.3   \n",
       "4            695     0          0.5         0  18       1          12    0.6   \n",
       "\n",
       "   mobile_wt  n_cores  ...  px_height  px_width   ram  sc_h  sc_w  talk_time  \\\n",
       "0        148        7  ...        942      1651  1704    17    13          2   \n",
       "1        102        8  ...         89      1538  2459    14     1         16   \n",
       "2        103        4  ...        125      1504  1799     5     2         11   \n",
       "3        164        7  ...        873      1394  1944     9     4          9   \n",
       "4        196        2  ...       1649      1829  2855    16    13          7   \n",
       "\n",
       "   three_g  touch_screen  wifi  price_range  \n",
       "0        1             0     1            1  \n",
       "1        1             1     0            0  \n",
       "2        1             0     1            0  \n",
       "3        1             1     0            0  \n",
       "4        1             1     1            1  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_df = pd.read_csv('train.csv')\n",
    "val_df = pd.read_csv('val.csv')\n",
    "print(train_df.shape)\n",
    "print(val_df.shape)\n",
    "\n",
    "\n",
    "train_df = pd.read_csv('train.csv')\n",
    "val_df = pd.read_csv('val.csv')\n",
    "\n",
    "x_train = train_df.drop(labels=[\"price_range\"], axis=\"columns\")\n",
    "feature_names = x_train.columns.values\n",
    "x_train = x_train.values\n",
    "y_train = train_df['price_range'].values\n",
    "\n",
    "x_test = val_df.drop(labels=[\"price_range\"], axis=\"columns\")\n",
    "x_test = x_test.values\n",
    "y_test = val_df['price_range'].values\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "Implement the Decision Tree algorithm (CART, Classification and Regression Trees) and trained the model by the given arguments, and print the accuracy score on the validation data. You should implement two arguments for the Decision Tree algorithm\n",
    "1. **criterion**: The function to measure the quality of a split. Your model should support `gini` for the Gini impurity and `entropy` for the information gain. \n",
    "2. **max_depth**: The maximum depth of the tree. If `max_depth=None`, then nodes are expanded until all leaves are pure. `max_depth=1` equals to split data once\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate the class in the leaf\n",
    "def class_counts(y_train) :\n",
    "    cl = 0\n",
    "    c1, c2 = 0, 0\n",
    "    for i in y_train:\n",
    "        if i == 1:\n",
    "            c1 += 1\n",
    "        else :\n",
    "            c2 += 1\n",
    "    if c1 == len(y_train) or c2 == len(y_train) :\n",
    "        return 3\n",
    "    if c1 >= c2 : \n",
    "        cl = 1\n",
    "    else : \n",
    "        cl = 0\n",
    "\n",
    "    return cl\n",
    "\n",
    "class Node:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.col = None\n",
    "        self.question = None\n",
    "        self.cl = None\n",
    "        self.depth = 0\n",
    "        self.true_branch = None\n",
    "        self.false_branch = None\n",
    "        self.leaf = False\n",
    "\n",
    "class DecisionTree():\n",
    "    def __init__(self, criterion='gini', max_depth=None):\n",
    "        self.max_depth = max_depth\n",
    "        self.criterion = criterion\n",
    "        self.x_data = None\n",
    "        self.y_data = None\n",
    "        self.tree = None\n",
    "        self.features = []\n",
    "        self.sample_weight = 1\n",
    "        self.ada = False    \n",
    "        self.max_features = 1\n",
    "        self.boostrap = False\n",
    "\n",
    "    #compute best decision for RandomForest\n",
    "    def find_low_error(self, lists, x_data, y_data) :\n",
    "        best_gain = None  \n",
    "        best_question = None\n",
    "        bestCol = None\n",
    "        #iterate through features\n",
    "        for col in lists: \n",
    "            val = []\n",
    "            for i in x_data : \n",
    "                val.append(i[col])\n",
    "            #iterate through every val in features\n",
    "            for i in val : \n",
    "                \n",
    "                true_col, false_col = [], []\n",
    "                true_branch, false_branch = [], []\n",
    "                true_w, false_w = [], []\n",
    "                question = i\n",
    "                if self.ada == True :\n",
    "                    true_branch = y_data[val >= question]\n",
    "                    true_col = [ index for index, i in enumerate(y_data[val >= question])]\n",
    "                    true_w = [self.sample_weight[index] for index, i in enumerate(y_data[val >= question])]\n",
    "\n",
    "                    false_branch = y_data[val < question]\n",
    "                    false_col = [ index for index, i in enumerate(y_data[val < question])]\n",
    "                    false_w = [self.sample_weight[index] for index, i in enumerate(y_data[val < question])]\n",
    "                else :\n",
    "                    true_branch = y_data[val >= question]\n",
    "                    false_branch = y_data[val < question]\n",
    "\n",
    "                if len(true_branch) == 0 or len(false_branch) == 0:\n",
    "                    continue\n",
    "                \n",
    "                #use gini\n",
    "                if self.criterion == 'gini' : \n",
    "                    if self.ada == False :\n",
    "                        gain = ((len(true_branch)/len(y_data)) * gini(true_branch) + (len(false_branch)/len(y_data)) *gini(false_branch))\n",
    "                    else :\n",
    "                        gain = ((len(true_branch)/len(y_data)) * gini(true_branch) + (len(false_branch)/len(y_data)) *gini(false_branch))\n",
    "                #use entropy\n",
    "                elif self.criterion == 'entropy' :\n",
    "                    \n",
    "                    gain = ((len(true_branch)/len(y_data)) * entropy(true_branch) + (len(false_branch)/len(y_data)) * entropy(false_branch))\n",
    "            \n",
    "                if best_gain == None or gain <= best_gain:\n",
    "                    bestCol = col\n",
    "                    best_gain, best_question = gain, question\n",
    "        \n",
    "        return bestCol, best_gain, best_question\n",
    "    #compute best decision\n",
    "    def find_best_decision(self, x_data, y_data):\n",
    "        best_gain = None  \n",
    "        best_question = None\n",
    "        bestCol = None  \n",
    "        current_gini = gini(y_data)\n",
    "        current_entropy = entropy(y_data)\n",
    "        # print(self.criterion)\n",
    "        if self.boostrap == True :\n",
    "            #randomly selecr features\n",
    "            lists =  np.random.choice(x_data.shape[1], self.max_features, replace = True)\n",
    "            bestCol, best_gain, best_question = self.find_low_error(lists, x_data, y_data)\n",
    "        else :\n",
    "            #iterate through features\n",
    "            for col in range(len(x_data[0])):\n",
    "                val = []\n",
    "                for i in x_data : \n",
    "                    val.append(i[col])\n",
    "                \n",
    "                #iterate through every val in features\n",
    "                for i in val : \n",
    "                    \n",
    "                    true_col, false_col = [], []\n",
    "                    true_branch, false_branch = [], []\n",
    "                    true_w, false_w = [], []\n",
    "                    question = i\n",
    "                    if self.ada == True :\n",
    "                        true_branch = y_data[val >= question]\n",
    "                        true_col = [ index for index, i in enumerate(y_data[val >= question])]\n",
    "                        true_w = [self.sample_weight[index] for index, i in enumerate(y_data[val >= question])]\n",
    "\n",
    "                        false_branch = y_data[val < question]\n",
    "                        false_col = [ index for index, i in enumerate(y_data[val < question])]\n",
    "                        false_w = [self.sample_weight[index] for index, i in enumerate(y_data[val < question])]\n",
    "                    else :\n",
    "                        true_branch = y_data[val >= question]\n",
    "                        false_branch = y_data[val < question]\n",
    "\n",
    "                    if len(true_branch) == 0 or len(false_branch) == 0:\n",
    "                        continue\n",
    "                    #use gini\n",
    "                    if self.criterion == 'gini' : \n",
    "                        if self.ada == False :\n",
    "                            gain = ((len(true_branch)/len(y_data)) * gini(true_branch) + (len(false_branch)/len(y_data)) *gini(false_branch))\n",
    "                        else :\n",
    "                            gain = ((len(true_branch)/len(y_data)) * gini(true_branch) + (len(false_branch)/len(y_data)) * gini(false_branch))\n",
    "                    #use entropy\n",
    "                    elif self.criterion == 'entropy' :\n",
    "                        gain = ((len(true_branch)/len(y_data)) * entropy(true_branch) + (len(false_branch)/len(y_data)) * entropy(false_branch))\n",
    "                        # print((len(true_branch)/len(y_data)))\n",
    "                \n",
    "                    if best_gain == None or gain <= best_gain:\n",
    "                        bestCol = col\n",
    "                        best_gain, best_question = gain, question\n",
    "        # if best_gain == None :\n",
    "        #     return None, None, None, None, None, None\n",
    "\n",
    "        #store chosen features, decison, col, val\n",
    "        self.features.append(bestCol)\n",
    "        best_val = [i[bestCol] for i in x_data]\n",
    "\n",
    "        x_true, x_false = x_data[best_val >= best_question, :], x_data[best_val < best_question, :]\n",
    "        y_true, y_false = y_data[best_val >= best_question], y_data[best_val < best_question]\n",
    "        \n",
    "        return bestCol, best_question, x_true, x_false, y_true, y_false\n",
    "\n",
    "    def build_tree(self, x_data, y_data, node):\n",
    "        #set limit depth\n",
    "        if node.depth == self.max_depth : \n",
    "            node.cl = class_counts(y_data)\n",
    "            return\n",
    "        #check wether the data in leaf is in same class\n",
    "        if class_counts(y_data) == 3 :\n",
    "            node.leaf = True\n",
    "            node.cl = y_data[0]\n",
    "            return\n",
    "\n",
    "        #find best decision\n",
    "        bestCol, best_question, x_true, x_false, y_true, y_false = self.find_best_decision(x_data, y_data)\n",
    "\n",
    "        #construct left and right node\n",
    "        node.col = bestCol\n",
    "        node.question = best_question\n",
    "        node.true_brach = Node()\n",
    "        node.false_brach = Node()\n",
    "\n",
    "        node.true_brach.depth = node.depth + 1\n",
    "        node.false_brach.depth = node.depth + 1\n",
    "\n",
    "        self.build_tree(x_true, y_true, node.true_brach)\n",
    "        self.build_tree(x_false, y_false, node.false_brach)\n",
    "    \n",
    "    #predict the y value\n",
    "    def prediction(self, x_data, node):\n",
    "        #reach max_depth then predict class \n",
    "        if node.depth == self.max_depth:\n",
    "            # print(node.cl)\n",
    "            return node.cl\n",
    "        #reach leaf then predict class \n",
    "        if node.leaf == True :\n",
    "            return node.cl\n",
    "\n",
    "        #decide which node to go \n",
    "        if x_data[node.col] >= node.question :\n",
    "            return self.prediction(x_data, node.true_brach)\n",
    "        else:\n",
    "            return self.prediction(x_data, node.false_brach)\n",
    "\n",
    "    #train the decision tree with data\n",
    "    def fit(self, x_data, y_data, ada=False, boostrap=False, max_features=1, sample_weight=1):\n",
    "        self.ada = ada\n",
    "        self.boostrap = boostrap\n",
    "        self.sample_weight = sample_weight\n",
    "        self.max_features = max_features\n",
    "        self.tree = Node()\n",
    "        self.tree.depth = 0\n",
    "        self.build_tree(x_data, y_data, self.tree)\n",
    "\n",
    "    #predict y data with builded decision tree\n",
    "    def predict(self,x_data):\n",
    "        pred = []\n",
    "        for x in x_data:\n",
    "            y = self.prediction(x, self.tree)\n",
    "            pred.append(y) \n",
    "        pred = np.asarray(pred)\n",
    "\n",
    "        return pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.1\n",
    "Using `criterion=gini`, showing the accuracy score of validation data by `max_depth=3` and `max_depth=10`, respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test-set accuarcy score:  0.92\n",
      "Test-set accuarcy score:  0.94\n"
     ]
    }
   ],
   "source": [
    "clf_depth3 = DecisionTree(criterion='gini', max_depth=3)\n",
    "clf_depth10 = DecisionTree(criterion='gini', max_depth=10)\n",
    "\n",
    "clf_depth3.fit(x_train, y_train)\n",
    "y_pred = clf_depth3.predict(x_test)\n",
    "print('Test-set accuarcy score: ', accuracy_score(y_test, y_pred))\n",
    "\n",
    "clf_depth10.fit(x_train, y_train)\n",
    "y_pred = clf_depth10.predict(x_test)\n",
    "print('Test-set accuarcy score: ', accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.2\n",
    "Using `max_depth=3`, showing the accuracy score of validation data by `criterion=gini` and `criterion=entropy`, respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test-set accuarcy score:  0.92\n",
      "Test-set accuarcy score:  0.8933333333333333\n"
     ]
    }
   ],
   "source": [
    "clf_gini = DecisionTree(criterion='gini', max_depth=3)\n",
    "clf_entropy = DecisionTree(criterion='entropy', max_depth=2)\n",
    "\n",
    "clf_gini.fit(x_train, y_train)\n",
    "y_pred = clf_gini.predict(x_test)\n",
    "print('Test-set accuarcy score: ', accuracy_score(y_test, y_pred))\n",
    "\n",
    "clf_entropy.fit(x_train, y_train)\n",
    "y_pred = clf_entropy.predict(x_test)\n",
    "print('Test-set accuarcy score: ', accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Note: Your decisition tree scores should over **0.9**. It may suffer from overfitting, if so, you can tune the hyperparameter such as `max_depth`\n",
    "- Note: You should get the same results when re-building the model with the same arguments,  no need to prune the trees\n",
    "- Hint: You can use the recursive method to build the nodes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "Plot the [feature importance](https://sefiks.com/2020/04/06/feature-importance-in-decision-trees/) of your Decision Tree model. You can get the feature importance by counting the feature used for splitting data.\n",
    "\n",
    "- You can simply plot the **counts of feature used** for building tree without normalize the importance. Take the figure below as example, outlook feature has been used for splitting for almost 50 times. Therefore, it has the largest importance\n",
    "\n",
    "![image](https://i2.wp.com/sefiks.com/wp-content/uploads/2020/04/c45-fi-results.jpg?w=481&ssl=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "features = {}\n",
    "\n",
    "#calculate features times\n",
    "for i in clf_depth10.features :\n",
    "    if features.get(feature_names[i]) == None :\n",
    "        features[feature_names[i]] = 1\n",
    "    else :\n",
    "        features[feature_names[i]] += 1\n",
    "        \n",
    "#use barh to produce chart\n",
    "cmap = cm.jet(np.linspace(0, 1, len(features)))\n",
    "plt.barh(*zip(*features.items()), color=cmap)\n",
    "plt.ylabel('Features')\n",
    "plt.xlabel('Times')\n",
    "plt.title('Feature Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "implement the AdaBooest algorithm by using the CART you just implemented from question 2 as base learner. You should implement one arguments for the AdaBooest.\n",
    "1. **n_estimators**: The maximum number of estimators at which boosting is terminated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#relabel the y val\n",
    "def relabel(y_data) :\n",
    "    for  i in range(len(y_data)) :\n",
    "        if y_data[i] == 0 :\n",
    "            y_data[i] = -1\n",
    "\n",
    "    return y_data \n",
    "\n",
    "#compute error\n",
    "def compute_error(y, y_pred, w_i):\n",
    "    \n",
    "    return (sum(w_i * (np.not_equal(y, y_pred)).astype(int)))\n",
    "\n",
    "#compute alpha\n",
    "def compute_alpha(error):\n",
    "    \n",
    "    return np.log((1 - error) / error) / 2\n",
    "\n",
    "#refresh weights\n",
    "def update_weights(w_i, alpha, y, y_pred):\n",
    "    # print(np.not_equal(y, y_pred))\n",
    "    return w_i * np.exp(alpha * (np.not_equal(y, y_pred)).astype(int))\n",
    "\n",
    "class AdaBoost():\n",
    "    def __init__(self, n_estimators):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.acc = 0\n",
    "        self.tree = None\n",
    "        self.alphas = []\n",
    "        self.clf = []   \n",
    "    \n",
    "    #fit decison tree with adaboost approach\n",
    "    def fit(self, x_data, y_data):  \n",
    "        y_data = y_data * 2 - 1\n",
    "        #iterate through n_estimaters\n",
    "        for m in tqdm(range(0, self.n_estimators)):\n",
    "            \n",
    "            if m == 0:\n",
    "                #initiate weight\n",
    "                w_i = np.ones(len(y_data)) * 1 / len(y_data)\n",
    "                \n",
    "                new_x = x_data\n",
    "            else:\n",
    "                #refresh weight and normalize\n",
    "                w_i = update_weights(w_i, alpha, y_data, y_pred)\n",
    "                w_i = w_i/sum(w_i)\n",
    "                #sample data with weight\n",
    "                idx = np.random.choice(len(x_data), len(x_data), replace = True, p = w_i)\n",
    "                new_x = x_data[idx]\n",
    "\n",
    "            #run decision tree\n",
    "            clf_depth3 = DecisionTree(criterion='gini', max_depth=1)\n",
    "            clf_depth3.fit(new_x, y_data, ada = True , sample_weight = w_i)\n",
    "            y_pred = clf_depth3.predict(x_data)\n",
    "            #relabel y_pred\n",
    "            y_pred = y_pred * 2 -1\n",
    "            #strore ecison tree\n",
    "            self.clf.append(clf_depth3)\n",
    "\n",
    "            error = compute_error(y_data, y_pred, w_i)\n",
    "\n",
    "            alpha = compute_alpha(error)\n",
    "            #store alpha\n",
    "            self.alphas.append(alpha)\n",
    "    #decide which class it is    \n",
    "    def result(self, pred) :\n",
    "        y_pred = []\n",
    "        for i in pred : \n",
    "            if i >= 0 :\n",
    "                y_pred.append(1)\n",
    "            else :\n",
    "                y_pred.append(0)\n",
    "\n",
    "        return np.asarray(y_pred)\n",
    "\n",
    "    #predict y val with sum result\n",
    "    def predict(self, x_data):\n",
    "        preds = np.zeros(len(x_data))\n",
    "\n",
    "        for m in range(self.n_estimators):\n",
    "            y_pred_m = self.clf[m].predict(x_data) * 2 - 1\n",
    "            y_pred_m = y_pred_m * self.alphas[m]\n",
    "            preds += y_pred_m\n",
    "            \n",
    "        #find predict class\n",
    "        y_pred = self.result(preds)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4.1\n",
    "Show the accuracy score of validation data by `n_estimators=10` and `n_estimators=100`, respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [05:50<00:00, 35.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test-set accuarcy score:  0.8933333333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [57:40<00:00, 34.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test-set accuarcy score:  0.9233333333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ada10 = AdaBoost(n_estimators=10)\n",
    "ada100 = AdaBoost(n_estimators=100)\n",
    "\n",
    "ada10.fit(x_train, y_train)\n",
    "y_pred = ada10.predict(x_test)\n",
    "print('Test-set accuarcy score: ', accuracy_score(y_test, y_pred))\n",
    "\n",
    "ada100.fit(x_train, y_train)\n",
    "y_pred = ada100.predict(x_test)\n",
    "print('Test-set accuarcy score: ', accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5\n",
    "implement the Random Forest algorithm by using the CART you just implemented from question 2. You should implement three arguments for the Random Forest.\n",
    "\n",
    "1. **n_estimators**: The number of trees in the forest. \n",
    "2. **max_features**: The number of random select features to consider when looking for the best split\n",
    "3. **bootstrap**: Whether bootstrap samples are used when building tree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#coutnt label for class find the most class\n",
    "def common_label(y_data) :\n",
    "    counter = Counter(y_data)\n",
    "    label = counter.most_common(1)[0][0]\n",
    "    return label\n",
    "\n",
    "class RandomForest():\n",
    "    def __init__(self, n_estimators, max_features, boostrap=True, criterion='gini', max_depth=None):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_features = math.ceil(max_features)\n",
    "        self.boostrap = boostrap\n",
    "        self.criterion = criterion\n",
    "        self.max_depth = max_depth\n",
    "        self.tree = []\n",
    "\n",
    "    def fit(self, x_data, y_data):\n",
    "        #iterate through the n_estimater\n",
    "        for i in tqdm(range(self.n_estimators)) :\n",
    "            clf_depth3 = DecisionTree(criterion='gini', max_depth=3)\n",
    "            #randomly choose data \n",
    "            idx = np.random.choice(len(x_data), len(x_data), replace = True)\n",
    "            new_x, new_y = x_data[idx], y_data[idx]\n",
    "            clf_depth3.fit(new_x, new_y, boostrap=self.boostrap, max_features=self.max_features)\n",
    "            self.tree.append(clf_depth3)\n",
    "\n",
    "    #predict label with randomforest\n",
    "    def predict(self, x_data):\n",
    "        tree_predicts = np.array([tree.predict(x_data) for tree in self.tree])\n",
    "        tree_predicts = np.swapaxes(tree_predicts, 0, 1)\n",
    "        y_pred = [common_label(tree_pred) for tree_pred in tree_predicts]\n",
    "        return np.array(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5.1\n",
    "Using `criterion=gini`, `max_depth=None`, `max_features=sqrt(n_features)`, showing the accuracy score of validation data by `n_estimators=10` and `n_estimators=100`, respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_10tree = RandomForest(n_estimators=10, max_features=np.sqrt(x_train.shape[1]))\n",
    "clf_100tree = RandomForest(n_estimators=100, max_features=np.sqrt(x_train.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:46<00:00,  4.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test-set accuarcy score:  0.87\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [06:48<00:00,  4.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test-set accuarcy score:  0.9133333333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "clf_10tree.fit(x_train, y_train)\n",
    "y_pred = clf_10tree.predict(x_test)\n",
    "print('Test-set accuarcy score: ', accuracy_score(y_test, y_pred))\n",
    "\n",
    "clf_100tree.fit(x_train, y_train)\n",
    "y_pred = clf_100tree.predict(x_test)\n",
    "print('Test-set accuarcy score: ', accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5.2\n",
    "Using `criterion=gini`, `max_depth=None`, `n_estimators=10`, showing the accuracy score of validation data by `max_features=sqrt(n_features)` and `max_features=n_features`, respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_random_features = RandomForest(n_estimators=10, max_features=np.sqrt(x_train.shape[1]))\n",
    "clf_all_features = RandomForest(n_estimators=10, max_features=x_train.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Note: Use majority votes to get the final prediction, you may get slightly different results when re-building the random forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:43<00:00,  4.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test-set accuarcy score:  0.89\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [03:17<00:00, 19.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test-set accuarcy score:  0.91\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "clf_random_features.fit(x_train, y_train)\n",
    "y_pred = clf_random_features.predict(x_test)\n",
    "print('Test-set accuarcy score: ', accuracy_score(y_test, y_pred))\n",
    "\n",
    "clf_all_features.fit(x_train, y_train)\n",
    "y_pred = clf_all_features.predict(x_test)\n",
    "print('Test-set accuarcy score: ', accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6. Train and tune your model on a real-world dataset\n",
    "Try you best to get higher accuracy score of your model. After parameter tuning, you can train your model on the full dataset (train + val).\n",
    "- Feature engineering\n",
    "- Hyperparameter tuning\n",
    "- Implement any other ensemble methods, such as gradient boosting. Please note that you **can not** call any package. Also, only ensemble method can be used. Neural network method is not allowed to used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_your_model(data):\n",
    "    ## Define your model and training \n",
    "    data_x_train = np.array(data.values)[:,0:(len(val_df.values[0])-1)]\n",
    "    data_y_train = np.array(data.values)[:,-1]\n",
    "    \n",
    "    ada175 = AdaBoost(n_estimators=175)\n",
    "    ada175.fit(data_x_train, data_y_train)    \n",
    "    # clf_depth3 = DecisionTree(criterion='gini', max_depth=3)\n",
    "    # clf_depth3.fit(data_x_train, data_y_train)\n",
    "    # y_pred = clf_depth3.predict(x_test)\n",
    "    # print('Test-set accuarcy score: ', accuracy_score(y_test, y_pred))\n",
    "    return ada175\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 175/175 [1:39:52<00:00, 34.24s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_df = pd.read_csv('train.csv')\n",
    "val_df = pd.read_csv('val.csv')\n",
    "data = pd.concat([train_df, val_df])\n",
    "data_x_train = data.values\n",
    "# print(data_x_train)\n",
    "\n",
    "my_model = train_your_model(data)\n",
    "test_df = pd.read_csv('x_test.csv')\n",
    "x_test = test_df.values\n",
    "# print(x_test)\n",
    "y_pred = my_model.predict(x_test)\n",
    "np.save('y_pred', y_pred)\n",
    "x = np.load('y_pred.npy')\n",
    "# print('Test-set accuarcy score: ', accuracy_score(train_y, y_pred))\n",
    "# print('Test-set accuarcy score: ', accuracy_score(train_y, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train = pd.read_csv('x_test.csv')\n",
    "# x_test = x_train.values\n",
    "# y_pred = my_model.predict(x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert y_pred.shape == (500, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supplementary\n",
    "If you have trouble to implement this homework, TA strongly recommend watching [this video](https://www.youtube.com/watch?v=LDRbO9a6XPU), which explains Decision Tree model clearly. But don't copy code from any resources, try to finish this homework by yourself! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DO NOT MODIFY CODE BELOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from sklearn.metrics import accuracy_score\n",
    "# # y_test = pd.read_csv('y_test.csv')['price_range'].values\n",
    "\n",
    "# print('Test-set accuarcy score: ', accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTree(criterion='entropy', max_depth=3) failed\n",
      "*** We will check your result for Question 3 manually *** (5 points)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:10<00:00,  1.09s/it]\n",
      "100%|██████████| 100/100 [01:57<00:00,  1.18s/it]\n",
      "100%|██████████| 10/10 [00:02<00:00,  4.29it/s]\n",
      "100%|██████████| 10/10 [00:02<00:00,  3.73it/s]\n",
      "100%|██████████| 10/10 [00:02<00:00,  3.96it/s]\n",
      "100%|██████████| 100/100 [00:26<00:00,  3.83it/s]\n",
      "100%|██████████| 10/10 [00:06<00:00,  1.59it/s]\n",
      "100%|██████████| 10/10 [00:05<00:00,  1.69it/s]\n",
      "100%|██████████| 10/10 [00:06<00:00,  1.59it/s]\n",
      "100%|██████████| 10/10 [00:06<00:00,  1.46it/s]\n",
      "100%|██████████| 10/10 [00:05<00:00,  1.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** We will check your result for Question 6 manually *** (20 points)\n",
      "Approximate score range: 42.5 ~ 67.5\n",
      "*** This score is only for reference ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def discrete_checker(score, thres, clf, name, x_train, y_train, x_test, y_test):\n",
    "    clf.fit(x_train, y_train)\n",
    "    y_pred = clf.predict(x_test)\n",
    "    if accuracy_score(y_test, y_pred) - thres >= 0:\n",
    "        return score\n",
    "    else:\n",
    "        print(f\"{name} failed\")\n",
    "        return 0\n",
    "\n",
    "\n",
    "def patient_checker(score, thres, CLS, kwargs, name,\n",
    "                    x_train, y_train, x_test, y_test, patient=10):\n",
    "    while patient > 0:\n",
    "        patient -= 1\n",
    "        clf = CLS(**kwargs)\n",
    "        clf.fit(x_train, y_train)\n",
    "        y_pred = clf.predict(x_test)\n",
    "        if accuracy_score(y_test, y_pred) - thres >= 0:\n",
    "            return score\n",
    "    print(f\"{name} failed\")\n",
    "    print(\"Considering the randomness, we will check it manually\")\n",
    "    return 0\n",
    "\n",
    "\n",
    "def load_dataset():\n",
    "    file_url = \"http://storage.googleapis.com/download.tensorflow.org/data/abalone_train.csv\"\n",
    "    df = pd.read_csv(\n",
    "        file_url,\n",
    "        names=[\"Length\", \"Diameter\", \"Height\", \"Whole weight\", \"Shucked weight\",\n",
    "               \"Viscera weight\", \"Shell weight\", \"Age\"]\n",
    "    )\n",
    "\n",
    "    df['Target'] = (df[\"Age\"] > 15).astype(int)\n",
    "    df = df.drop(labels=[\"Age\"], axis=\"columns\")\n",
    "\n",
    "    train_idx = range(0, len(df), 10)\n",
    "    test_idx = range(1, len(df), 20)\n",
    "\n",
    "    train_df = df.iloc[train_idx]\n",
    "    test_df = df.iloc[test_idx]\n",
    "\n",
    "    x_train = train_df.drop(labels=[\"Target\"], axis=\"columns\")\n",
    "    feature_names = x_train.columns.values\n",
    "    x_train = x_train.values\n",
    "    y_train = train_df['Target'].values\n",
    "\n",
    "    x_test = test_df.drop(labels=[\"Target\"], axis=\"columns\")\n",
    "    x_test = x_test.values\n",
    "    y_test = test_df['Target'].values\n",
    "    return x_train, y_train, x_test, y_test, feature_names\n",
    "\n",
    "\n",
    "score = 0\n",
    "\n",
    "data = np.array([1, 2])\n",
    "if abs(gini(data) - 0.5) < 1e-4:\n",
    "    score += 2.5\n",
    "else:\n",
    "    print(\"gini test failed\")\n",
    "\n",
    "if abs(entropy(data) - 1) < 1e-4:\n",
    "    score += 2.5\n",
    "else:\n",
    "    print(\"entropy test failed\")\n",
    "\n",
    "x_train, y_train, x_test, y_test, feature_names = load_dataset()\n",
    "\n",
    "score += discrete_checker(5, 0.9337,\n",
    "                          DecisionTree(criterion='gini', max_depth=3),\n",
    "                          \"DecisionTree(criterion='gini', max_depth=3)\",\n",
    "                          x_train, y_train, x_test, y_test\n",
    "                          )\n",
    "\n",
    "score += discrete_checker(2.5, 0.9036,\n",
    "                          DecisionTree(criterion='gini', max_depth=10),\n",
    "                          \"DecisionTree(criterion='gini', max_depth=10)\",\n",
    "                          x_train, y_train, x_test, y_test\n",
    "                          )\n",
    "\n",
    "score += discrete_checker(2.5, 0.9096,\n",
    "                          DecisionTree(criterion='entropy', max_depth=3),\n",
    "                          \"DecisionTree(criterion='entropy', max_depth=3)\",\n",
    "                          x_train, y_train, x_test, y_test\n",
    "                          )\n",
    "\n",
    "print(\"*** We will check your result for Question 3 manually *** (5 points)\")\n",
    "\n",
    "score += patient_checker(\n",
    "    7.5, 0.91, AdaBoost, {\"n_estimators\": 10},\n",
    "    \"AdaBoost(n_estimators=10)\",\n",
    "    x_train, y_train, x_test, y_test\n",
    ")\n",
    "\n",
    "score += patient_checker(\n",
    "    7.5, 0.87, AdaBoost, {\"n_estimators\": 100},\n",
    "    \"AdaBoost(n_estimators=100)\",\n",
    "    x_train, y_train, x_test, y_test\n",
    ")\n",
    "\n",
    "score += patient_checker(\n",
    "    5, 0.91, RandomForest,\n",
    "    {\"n_estimators\": 10, \"max_features\": np.sqrt(x_train.shape[1])},\n",
    "    \"RandomForest(n_estimators=10, max_features=sqrt(n_features))\",\n",
    "    x_train, y_train, x_test, y_test\n",
    ")\n",
    "\n",
    "score += patient_checker(\n",
    "    5, 0.91, RandomForest,\n",
    "    {\"n_estimators\": 100, \"max_features\": np.sqrt(x_train.shape[1])},\n",
    "    \"RandomForest(n_estimators=100, max_features=sqrt(n_features))\",\n",
    "    x_train, y_train, x_test, y_test\n",
    ")\n",
    "\n",
    "score += patient_checker(\n",
    "    5, 0.92, RandomForest,\n",
    "    {\"n_estimators\": 10, \"max_features\": x_train.shape[1]},\n",
    "    \"RandomForest(n_estimators=10, max_features=n_features)\",\n",
    "    x_train, y_train, x_test, y_test\n",
    ")\n",
    "\n",
    "print(\"*** We will check your result for Question 6 manually *** (20 points)\")\n",
    "print(\"Approximate score range:\", score, \"~\", score + 25)\n",
    "print(\"*** This score is only for reference ***\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "4d017183e2d001f3adde55206bad98abad981a365b43064e490592d198e63cac"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
